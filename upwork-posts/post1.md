Experienced Generative AI Engineer (RAG, Vector Retrieval, Scaling)


Summary
Description
Weâ€™re building a Generative AI platform for a very large enterprise client. Right now, the project is in MVP stage, but the roadmap is to scale it into a fully enterprise-grade solution.

Our team is composed of experienced developers with some GenAI experience, but weâ€™re looking for a true expert in AI system design and optimization to advise, guide, and potentially implement backend improvements.

If youâ€™ve built and optimized AI-powered products that went into production and scaled to many users, weâ€™d like to work with you.

---

Whatâ€™s Built So Far
The MVP backend is live and functional, with:

* RAG / Vector storage & retrieval (Pinecone)
* Web Search integration
* Prompt-chaining workflows

---

Immediate Needs
We need help with:

* RAG Optimization: Improve relevance of chunk retrieval from Pinecone. Current queries arenâ€™t returning the most comprehensive results.
* Scaling AI Calls: Solve token limit (429) errors and concurrency issues when many users make multiple API calls at once.
* Performance Improvements: Diagnose weak points in AI output and recommend improvements.
* Web Search Strategy: Right now we rely on ChatGPTâ€™s default web search tool, but may want to roll out our own custom solution.

---

Our Tech Stack

* Backend: Python, FastAPI
* LLM: ChatGPT (open to evaluating others)
* Vector Database: Pinecone
* SQL Database: Postgres (Supabase)
* Frontend: React
* Hosting: Render

---

Qualifications (Must-Haves)

* Computer Science degree (or equivalent proven expertise)
* Deep experience building Generative AI systems (preferably enterprise-grade apps)
* Strong background in RAG pipelines, vector search optimization, and scaling AI systems
* Expertise with Python and modern AI dev tools (Cursor, LangChain, LlamaIndex, etc.)
* Strong communication skills in English

Nice-to-Have

* Experience with caching, batching, and queueing strategies for LLMs
* Familiarity with Supabase/Postgres performance tuning
* Prior experience optimizing OpenAI API usage at scale

---

Why Work With Us

* This project is backed by a large enterprise client (big opportunity if MVP succeeds)
* Initial scope: advisory + implementation support
* Future scope: Potential for a long-term contract to scale into a secure, enterprise-grade solution

---

ðŸ’¡ If youâ€™re the type of engineer who loves solving hard AI infrastructure problems and scaling GenAI products, weâ€™d love to talk to you.

Please include in your proposal:

* Links or details of similar GenAI projects youâ€™ve worked on
* A brief note on your experience with Pinecone and scaling LLM API usage

Less than 30 hrs/week
Hourly
Less than 1 month
Duration
Expert
I am willing to pay higher rates for the most experienced freelancers
$30.00

-

$60.00

Hourly
Contract-to-hire opportunity
This lets talent know that this job could become full time.
Learn more
Project Type: One-time project
You will be asked to answer the following questions when submitting a proposal:

One of our issues is improving our RAG vector retrieval. We want retrieval to be comprehensive and relevant but also not blow out context length. How do you usually go bout optimizing vector retrieval? Explain an experience where you've done this.
Another issue is that we're running into token limit issues with the openAI API. We can't run concurrent API calls because we hit TPM limits. How would you go about solving this? Explain experiences you've had solving this type of issue.
On a scale of 1-10 how good are you backend API developement
On a scale of 1-10 how experienced do you consider yourself as a GenAI engineer?
Describe your recent experience with similar projects